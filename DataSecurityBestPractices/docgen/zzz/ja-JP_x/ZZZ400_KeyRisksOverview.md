## 主要リスク概要：LLMにおけるデータセキュリティ

大規模言語モデル（LLM）の急速な採用は、データセキュリティに前例のないリスクをもたらしています。本ホワイトペーパーは、データセキュリティに関する当社の見解を示しており、プロジェクトで作成されたその他のコンテンツに依拠しています。組織がLLMへの依存を拡大するにつれて、機密情報の保護、コンプライアンスの確保、プライバシー、および進化する脅威の軽減に関する課題は一層深刻になります。本セクションでは、LLMに関連する重要なデータリスクを検討し、2025年におけるデータの完全性とセキュリティを守るための先見的な解決策を提案します。

### 1. 機密情報の漏洩

LLMは広範なデータセットで訓練されており、その中には意図せずに機密情報や個人を特定できる情報（PII）が含まれている場合があります。厳格なデータ準備を行っても、モデルは機密情報を「記憶」し、出力にそれを再現することがあり、重大なリスクをもたらします。

#### 主なリスク:
- データの記憶：推論中に意図しない情報漏洩を引き起こす、機密または独自情報の保持。
- 安全でないデータ出力：不十分に設計された出力処理メカニズムは、機密情報や企業秘密を露出させる可能性があります。
#### 影響:
- GDPRやCCPAなどのデータ保護規制に違反すること。
- プライバシー侵害による評判被害。
- AIシステムに対するユーザーの信頼の低下。
#### 緩和戦略:
- 匿名化：直接識別子を削除し、k-匿名性や合成データ生成などの高度な技術を使用する。
- 差分プライバシー：統計的なノイズをデータセットや出力に注入し、ユーティリティを保持しながら機微な情報を隠す。
- 出力フィルタリング：モデルの出力が共有される前に、潜在的にセンシティブな内容を検出し削除するための強力なフィルターを実装する。
  ​
### 2. データ漏洩

LLMは大規模なデータセットを扱うため、サイバー犯罪者にとって利益を狙いやすい標的となります。侵害は、不十分なストレージセキュリティ、弱い暗号化の実践、またはサプライチェーンの脆弱性から生じる可能性があります。

#### 主なリスク:
- セキュリティの誤設定：データストレージが公開されていると、攻撃者に侵入経路を提供します。
- 暗号化されていないストレージ：保存データの保護に失敗すると、盗難のリスクが高まります。
- サプライチェーンの脆弱性：オープンソースモデルおよびサードパーティの依存関係は、攻撃者の侵入口となり得る。
#### 影響:
- 知的財産の喪失、財務上の罰則、運用の混乱。
- 個人を特定できる情報（PII）や独自の業務データを含む機密トレーニングデータセットの露出。
#### 緩和戦略:
- 暗号化：AES-256やTLS 1.3などの堅牢なアルゴリズムを使用して、保存データと転送中データを暗号化します。
- ゼロトラストアーキテクチャ：認証および認可されたユーザーのみにデータアクセスを制限する。
- サプライチェーンセキュリティ：サードパーティコンポーネントの定期的な監査を実施し、オープンソースの依存関係における脆弱性を監視するツールを使用します。
  ​
### 3. データおよびモデルポイズニング

攻撃者はトレーニングパイプラインに悪意のあるデータを注入し、LLMの整合性を損なう可能性があります。ポイズニング攻撃は、モデルの挙動にバイアスをかけたり、後で悪用される可能性のある脆弱性を導入したりすることを目的としています。

#### 主なリスク:
- トレーニングデータの破損：モデルの出力に影響を与える欺瞞的なデータの挿入。
- 隠されたバックドア：特定の応答を引き出すためにモデルに埋め込まれたトリガーフレーズ。
#### 影響:
- バイアスのかかった有害なコンテンツの生成は、ユーザーの信頼を損なう。
- モデルに依存する下流アプリケーションのセキュリティ脆弱性。
#### 緩和戦略:
- データ検証とクレンジング：トレーニングデータの異常を検出し除去するための自動化パイプラインを確立する。
- 敵対的訓練：モデルの耐性を高めるために、訓練中に敵対的サンプルを含める。
- 動作監視：中毒を示す可能性のある予期しないパターンや挙動について、モデルの出力を継続的に分析する。
  ​
### 4. 不正アクセスとモデルの窃盗

堅牢なアクセス制御がなければ、LLMは盗難や不正使用のリスクにさらされます。悪意のある者は、広範なAPI操作を通じて独自モデルをクローンしたり、知的財産を盗もうとする可能性があります。

#### 主なリスク:
- APIの悪用：攻撃者はAPIを操作して機密情報を抽出したり、クエリでシステムを圧倒したりします。
- モデル抽出：ブラックボックス技術を使用して、攻撃者はLLMの動作を複製し、ほぼ同一のクローンを作成します。
#### 影響:
- 競争優位性と収益の喪失。
- LLMの脆弱性を悪用する、規制されていない潜在的に有害なクローンの作成。
#### 緩和戦略:
- アクセス制御：多要素認証（MFA）、役割ベースのアクセス制御（RBAC）、および詳細な権限を適用する。
- レート制限：抽出試行を抑制するために、APIコールの頻度と量を制限する。
- ウォーターマーキング：モデルに識別子を埋め込み、モデルの無断配布を追跡できるようにする。ただし、識別子はそれ以外の場合の脅威に対しては限定的な緩和策を提供する。
  ​
### 5. 敵対的攻撃

敵対的入力はモデルの脆弱性を悪用し、有害な出力を引き起こしたり、機密データの抽出をもたらしたりします。これらの攻撃には、プロンプトインジェクション、データ反転、および出力操作が含まれます。
![fig2](images/fig2.png)
[https://www.labellerr.com/blog/what-are-adversarial-attacks-in-machine-learning-and-how-can-you-prevent-them/](https://www.labellerr.com/blog/what-are-adversarial-attacks-in-machine-learning-and-how-can-you-prevent-them/)
##### 図2: 機械学習における攻撃と予防措置

#### 主なリスク:
- プロンプトインジェクション：情報を抽出したり動作を操作したりするために、セーフガードを回避する入力を作成すること。
- モデル反転: モデルの出力を分析することで機密トレーニングデータを再構築すること。
- 出力操作：有害または意図しないコンテンツの生成。
#### 影響:
- トレーニングデータに埋め込まれた個人情報の漏洩。
- 下流のアプリケーションにおける誤情報や悪意ある行動の増幅。
#### 緩和戦略:
- 入力検証：悪意のある入力をフィルタリングするために堅牢なサニタイズ手法を採用する。
- 敵対的訓練：LLMを敵対的事例で訓練し、堅牢性を向上させる。
- 出力監視：有害または不適切な出力を検出しブロックするために自動化システムを使用します。
  ​
### 6. データセキュリティにおける将来の考慮事項

#### 新たに浮上する課題

LLMが重要な業務により深く統合されるにつれて、新たな課題が浮上します：
- プライバシーリスク：ユーザーのやり取りに対する依存度が高まることで、機微な情報への曝露が増加します。
- 動的な脅威の状況：攻撃者は既知および予期せぬ脆弱性の両方を利用し、絶えず革新を続けています。
- AIエージェントとマルチエージェントシステム：計画立案、推論、そして行動といった複雑なシステムであり、攻撃対象領域を大幅に拡大する。
  ​
#### 将来を見据えたソリューション

- プライバシー強化技術（PETs）：フェデレーテッドラーニング、セキュアマルチパーティ計算、およびホモモルフィック暗号化は、機密データの保護において重要な役割を果たします。
- AIガバナンスフレームワーク：倫理的なAI利用のための明確な指針を確立し、グローバルな基準や規制との整合性を確保します。
- 継続的な監視と更新：進化する脅威に対応するために、セキュリティ対策を定期的に監査し更新する。

